package com.cookiek.commenthat.service;

import com.cookiek.commenthat.domain.YoutubeRaw;
import com.cookiek.commenthat.domain.YoutubeTrend;
import com.cookiek.commenthat.repository.YoutubeRawRepository;
import com.cookiek.commenthat.repository.YoutubeTrendRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.openkoreantext.processor.OpenKoreanTextProcessorJava;
import org.openkoreantext.processor.tokenizer.KoreanTokenizer;
import org.springframework.core.io.ClassPathResource;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;
import scala.collection.Seq;

import java.io.*;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.time.LocalDate;
import java.util.*;
import java.util.stream.Collectors;

@Slf4j
@Service
@RequiredArgsConstructor
public class YoutubeTrendAnalysisService {

    private final YoutubeRawRepository youtubeRawRepository;
    private final YoutubeTrendRepository youtubeTrendRepository;

    private static final String CACHE_FILE_TEMPLATE = "youtube_tokens_%s_%s.txt";
    private static final String STOPWORDS_FILE = "stopwords.txt";  // resources í´ë”ì— ìœ„ì¹˜

    // ë§¤ì¼ ì˜¤ì „ 2ì‹œì— ì‹¤í–‰
//    @Scheduled(cron = "0 0 2 * * *", zone = "Asia/Seoul")
    public void analyzeTrends() {
        LocalDate periodEndDate = LocalDate.now();
        LocalDate periodStartDate = periodEndDate.minusWeeks(1);

        String cacheFileName = String.format(CACHE_FILE_TEMPLATE,
                periodStartDate.toString().replaceAll("-", ""),
                periodEndDate.toString().replaceAll("-", "")
        );

        log.info("âœ… íŠ¸ë Œë“œ ë¶„ì„ ì‹œì‘: ê¸°ê°„ {} ~ {}", periodStartDate, periodEndDate);
        log.info("ğŸ“‚ ìºì‹œ íŒŒì¼ëª…: {}", cacheFileName);

        List<String> tokens;

        if (Files.exists(Paths.get(cacheFileName))) {
            log.info("ğŸ“‚ ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•©ë‹ˆë‹¤. íŒŒì¼ì—ì„œ í† í°ì„ ì½ìŠµë‹ˆë‹¤: {}", cacheFileName);
            try {
                tokens = Files.readAllLines(Paths.get(cacheFileName))
                        .stream()
                        .map(String::trim)
                        .filter(s -> !s.isEmpty())
                        .collect(Collectors.toList());
            } catch (IOException e) {
                throw new RuntimeException("ìºì‹œ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨", e);
            }
        } else {
            log.info("ğŸ†• ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í† í°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.");

            List<YoutubeRaw> lastWeekVideos = youtubeRawRepository.findAllByTimeBetween(
                    periodStartDate.atStartOfDay(),
                    periodEndDate.atStartOfDay()
            );
            log.info("ğŸ“¥ ìˆ˜ì§‘ëœ ì˜ìƒ ìˆ˜: {}", lastWeekVideos.size());

            Set<String> excludedPos = Set.of(
                    "Josa", "Eomi", "PreEomi", "Conjunction", "Determiner",
                    "Adverb", "Suffix", "Verb", "Exclamation", "Foreign",
                    "Number", "Punctuation",
                    "Adjective", "KoreanParticle", "Alpha",
                    "ScreenName", "Email", "URL", "CashTag", "Emoji"
            );

            List<String> allTokens = new ArrayList<>();

            for (YoutubeRaw video : lastWeekVideos) {
                String text = video.getTitle() + " " + video.getDes();
                CharSequence normalized = OpenKoreanTextProcessorJava.normalize(text);
                Seq<KoreanTokenizer.KoreanToken> tokenSeq = OpenKoreanTextProcessorJava.tokenize(normalized);
                List<KoreanTokenizer.KoreanToken> tokenList = scala.collection.JavaConverters.seqAsJavaList(tokenSeq);

                Set<String> tokenSet = tokenList.stream()
                        .filter(token -> !excludedPos.contains(token.pos().toString()))
                        .map(token -> {
                            String word = token.text().trim();
                            if (token.pos().toString().equals("Hashtag") && word.startsWith("#") && word.length() > 1) {
                                word = word.substring(1);  // âœ… # ì œê±°
                            }
                            return word;
                        })
                        .filter(word -> word.length() > 1)
                        .filter(word -> word.matches("^[ê°€-í£a-zA-Z]+$"))
                        .collect(Collectors.toSet());  // ğŸš© ì¤‘ë³µ ì œê±°

                allTokens.addAll(tokenSet);
            }

            log.info("âœ… í† í°í™” ì™„ë£Œ (ë¶ˆìš©ì–´ ì œê±° ì „): {}ê°œ", allTokens.size());

            // âœ… ì‚¬í›„ ë³‘í•© ë¡œì§
            List<String> mergedTokens = new ArrayList<>();
            for (int i = 0; i < allTokens.size(); i++) {
                String current = allTokens.get(i);
                if (i < allTokens.size() - 1) {
                    String next = allTokens.get(i + 1);
                    if (current.length() >= 2 && next.length() == 1) {
                        String merged = current + next;
                        mergedTokens.add(merged);
                        log.debug("ğŸ”— ë³‘í•©ë¨: {} + {} â†’ {}", current, next, merged);
                        i++;
                        continue;
                    }
                }
                mergedTokens.add(current);
            }

            log.info("ğŸ”— ë³‘í•© í›„ í† í° ê°œìˆ˜: {}", mergedTokens.size());

            try (BufferedWriter writer = new BufferedWriter(new FileWriter(cacheFileName))) {
                for (String word : mergedTokens) {
                    writer.write(word);
                    writer.newLine();
                }
                log.info("ğŸ“ ë³‘í•©ëœ í† í°ì„ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤: {}", cacheFileName);
            } catch (IOException e) {
                throw new RuntimeException("ìºì‹œ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨", e);
            }

            tokens = mergedTokens;
        }

        // ë¶ˆìš©ì–´ ì œê±°
        Set<String> stopWords = loadStopWords();
        List<String> filteredTokens = tokens.stream()
                .filter(word -> !stopWords.contains(word))
                .collect(Collectors.toList());

        log.info("ğŸš« ë¶ˆìš©ì–´ ì œê±° í›„ ìµœì¢… ëª…ì‚¬ ê°œìˆ˜: {}", filteredTokens.size());

        // ë‹¨ì–´ ì¹´ìš´íŠ¸
        Map<String, Long> wordCountMap = filteredTokens.stream()
                .collect(Collectors.groupingBy(w -> w, Collectors.counting()));

        log.info("ğŸ”¢ ê³ ìœ  ë‹¨ì–´ ê°œìˆ˜: {}", wordCountMap.size());

        // ìƒìœ„ 1000ê°œ ì¶”ì¶œ
        List<Map.Entry<String, Long>> topWords = wordCountMap.entrySet().stream()
                .sorted(Map.Entry.<String, Long>comparingByValue().reversed())
                .limit(1000)
                .collect(Collectors.toList());

        log.info("ğŸ† ìƒìœ„ 50ê°œ ë‹¨ì–´:");
        topWords.forEach(entry ->
                log.info("   {}: {}íšŒ", entry.getKey(), entry.getValue())
        );

        // ì €ì¥
        for (Map.Entry<String, Long> entry : topWords) {
            YoutubeTrend trend = new YoutubeTrend();
            trend.setWord(entry.getKey());
            trend.setCount(entry.getValue().intValue());
            trend.setPeriodStart(periodStartDate.atStartOfDay());
            trend.setPeriodEnd(periodEndDate.atStartOfDay());
            youtubeTrendRepository.save(trend);
            log.info("ğŸ’¾ ì €ì¥ ì™„ë£Œ: {} ({})", entry.getKey(), entry.getValue());
        }

        log.info("ğŸ‰ íŠ¸ë Œë“œ ë¶„ì„ ì™„ë£Œ âœ…");
    }

    /**
     * ë¶ˆìš©ì–´ ëª©ë¡ì„ íŒŒì¼ì—ì„œ ì½ì–´ì˜¤ê¸°
     */
    private Set<String> loadStopWords() {
        try {
            ClassPathResource resource = new ClassPathResource(STOPWORDS_FILE);
            List<String> lines = Files.readAllLines(resource.getFile().toPath());
            Set<String> stopWords = lines.stream()
                    .map(String::trim)
                    .filter(s -> !s.isEmpty())
                    .collect(Collectors.toSet());
            log.info("ğŸ›‘ ë¶ˆìš©ì–´ {}ê°œ ë¡œë“œ ì™„ë£Œ", stopWords.size());
            return stopWords;
        } catch (IOException e) {
            throw new RuntimeException("ë¶ˆìš©ì–´ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: " + STOPWORDS_FILE, e);
        }
    }
}
